

## Gradient descent
X is input a MN matrix
M is the examples
N is the attributes
X1..Xn is an input vector
y = (y1, .., yM) is a vector of target values 

linear regression: oi = w * xi 
where w is the weight

w0 is seen as a row where each element is 1, to simplify formular. 


objective is to find the difference between yi and oi, or the squared difference. 
we want to find w such that the difference between o and y are minimal

error function based on w.  E(w)
See slides updating the weights

alpha = learning rate 
how much to change values of w per iteration.



## Neural networks



## Back propagation
## Discrete Attributes
## Expressive Power
